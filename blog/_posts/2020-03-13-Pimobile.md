---
layout: post
title: The PI-Mobile, an autonomous lego car powered by Raspberry Pi 
---

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-145347384-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-145347384-1');
</script>

<style TYPE="text/css">
code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry
    }
});
MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML-full"></script>

When we started talking about this project with [Matthias](https://www.linkedin.com/in/matthias-cremieux-4b1222153/)), we knew that both building a Lego motorized car, and learning to drive with real-life Deep Reinforcement Learning was [possible](https://arxiv.org/pdf/1807.00412.pdf). However, we wanted to do both at the same time. We recently acquired a Raspberry Pi 4b and knew that the power limitations set by the previous model vanished. However, we didn't know exactly how to fit everything into a lego car, and how to train it properly. We came up with 2 cars and 2 methods of learning :

* a first model (that looks terrible), driving with a Deep Reinforcement Learning Agent
* a much better car, driving with an Imitation Learning algorithm

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/car-v1.jpeg">
</p>

<center>
<em>
The first car we built, not the better looking...
</em>
</center>

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/car-v2.jpeg">
</p>

<center>
<em>
The Pi-Mobile V2, based on a Lego Technic Porsche Model
</em>
</center>

The first one drove on an inside track built in my home, while the second drove outside.

# PI-Mobile V1 

## Assembling the car

The first step of building a lego car is building its structure and deciding the location of each component in the car to make it as compact as possible. Considering that we did not look at any car blueprint or tutorial to build it, it was almost like reinventing the car in a way, and consequently, the car we built is not the prettiest of all time! However, it does drive, although it is not that fast nor that smooth. 

### The structure

We decided to choose a 3-engines solution: two at the back of the car that would power each wheel separately, and one at the front, that would be powering the steering system that we would've built.

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/components.jpeg">
</p>

<center>
<em>
From top left to bottom right : Lego Mindstorm cables, RaspberyPi 4b + BrickPi, 8 AA batteries, RasbperryPi 3b + Lithium battery + fan, 2 Large Lego Mindstorm motors, 1 Medium Lego Mindsotrm rotor
</em>
</center>

We chose to separate the car's electronic system in two halves: one half would be the core of the car and would be linked to the motors, power them, and would take care of the DRL part of the project, and the second half would be here to cool the first part, as all the operations listed above are energy-consuming. 

The core part consists of a Raspberry Pi 4b (we chose this device to be the center of all the operations because of its vast specifications compared to the Raspberry Pi 3b) and a card from [Dexter Industries](https://www.dexterindustries.com/) that manages the link with the lego technics. In contrast, the second part consists of a battery powering a Raspberry Pi 3 that powers a small fan, installed in between the two cards of the car's core. While using a Raspberry Pi 3 to power a fan seems a bit of an overkill; it also allows us convenient flexibility in programming, were we to decide to use a Master/slave architecture to compute the DRL part if the RP4 capped its power due to overheating. 

We then used a raspbian image, unaltered, with the convenient package Vncviewer, which allows us to access the raspbian of the pi via ssh protocol. From there, we can launch our script to make the car move.

We also used a raspberry camera on top of the car. This camera was the only input our car would use to drive around. 

### The track 

The PI-Mobile V1 trained on a track we built inside my house. It is made of paper and is 8 meters long. However, it has a bridge in the middle, a bridge the car could never learn to pass properly (because it struggles to go only forward, the fault to a wrong steering conception). Because of this, we split the track into 2 parts: a training part and a testing part :

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/track-1.png">
</p>

## The learning algorithm - DQL

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/graph-agent.png">
</p>

We used a Deep Q-learning agent to train our car to drive, with some miners modifications: human control. Since the agent is not interacting with a simulation but with the real world, we are the one telling it when to stop, or when it's good or bad. We set up several keys: one to pause the current episode, one to finish it when the car is doing something bad, and finally, one to end the episode when the car reaches the end of the track. The algorithm is available below : 

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/algo-1.png">
</p>

A more detailled explanation of Deep Reinforcement Learning is available on a previous blog post and paper.

Regarding the reward, we used a simple function: 1 at every step, except when the car fails (-1) or when it reaches the end of the trach (time spent on the track). The goal is to drive correctly and as fast as it could. 

```java
public double reward(String key, double time_since_start) {
    double reward = 0;
    
    if (key == stop) {
        reward = -1;
    } else if (key == end) {
        reward = (10. - time_since_start)/100.;
    } else {
        reward = 1;
    }
    
    return reward;
}
```

Overall, our car learned to drive on the testing track in less than 50 episodes (around an hour and a half). We tracked the distance made autonomously on the testing track every 5 training episodes : 

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/distance-v1.png">
</p>

<center>
<em>
Distance autonomously driven by the car on the testing track
</em>
</center>

An example of a training episode is available below : 

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/car-v1.gif">
</p>

# PI-Mobile V2 

Now that we knew that our car could learn to drive on its own, we wanted to make some modifications: 

<table>
  <thead>
    <tr>
      <th>Car V1</th>
      <th>Modifications</th>
    </tr>
  </thead>
  <tfoot>
    <tr>
    <td>Car's structure was not robust enough</td>
    <td> Started from an existing Lego Technic car </td>
    </tr>
  </tfoot>
  <tbody>
  <tr>
    <td>The fan system was taking a lot of place</td>
    <td>Remove the fan and the Raspberry 3</td>
  </tr>
    <tr>
      <td>The DRL agent was not necesseraly the best solution</td>
      <td> Use of an Imitation Learning algorithm </td>
    </tr>
    <tr>
      <td>The track was too small</td>
      <td> Train the car on a bigger track outside </td>
    </tr>
  </tbody>
</table>

## Structure 

We kept the same main composants (Raspberry Pi 4, BrickPi and 3 motors) but changed the rest of the car. We used a [Lego Technic Porsche 911](https://www.amazon.fr/LEGO-Technic-Porsche-42096-construction/dp/B07FP6QNQ7/ref=asc_df_B07FP6QNQ7/?tag=googshopfr-21&linkCode=df0&hvadid=314587880581&hvpos=&hvnetw=g&hvrand=10330215336837655036&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9056237&hvtargid=aud-882886963594:pla-586194956307&psc=1&tag=&ref=&adgrpid=66274120487&hvpone=&hvptwo=&hvadid=314587880581&hvpos=&hvnetw=g&hvrand=10330215336837655036&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9056237&hvtargid=aud-882886963594:pla-586194956307) model as a base for the rest of the car. 

Our goal was to use the following blueprint : 

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/blueprint1.png">
</p>

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/blueprint2.png">
</p>

However, we had to make some modifications, mainly because of 2 things: 

* The suspension system at the back of the car was not compatible with our Large motors
* The case used for the Raspberry Pi + BrickPi  was not made for a Raspberry Pi 4, and therefore was not as robust as it should have been.

We removed the V6 engine and replaced it by the Raspberry Pi and the batteries. We also made some modifications to insert the Lego Rotor at the front, and the 2 Lego motors at the back. Using this as a baseline allows the car to be much more stable and with a more precise steering system. The first version also had a tendency to break at the middle of the chassis, this doesn't happen anymore. 

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/car-v2-debut.jpeg">
</p>

<center>
<em>
Chassis of the PI-Mobile V2, with the 2 motors and the Raspberry Pi/BrickPi in the middle.</em>
</center>

## The track 

We wanted to be a bit more ambitious, and step aside from the small inside track we used for the first car. We used a small pathway around my home as a circuit. This pathway lies inside a large garden, making it easy to define where the car should drive, and makes a loop: exactly what we needed.

<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/track-2.png">
</p>

<center>
<em>
Track used to gather data for the PI-Mobile V2. Track is around 400 meters long.</em>
</center>

We would then use the rest of the pathway for testing our model. An example of such pathway is available below : 


<p align="center">
  <img src="{{ site.url }}/imgs/2020-03-13-Pimobile/ex-road.jpeg">
</p>

<center>
<em>
Example of the road used for the second track</em>
</center>

Now that we have a new car and a new track, it is time to talk about the models we used to make our car autonomous.

## Imitation Learning and Conditional Imitation Learning

### Imitation Learning

Let $\mathcal{O}$ be the set of possible observations. In our case, it is the input from our camera. Let $\mathcal{A}$ the set of possible actions, here it could be turn left or right or change the speed. Let's suppose we have a dataset $\mathcal{D}$ made of observation-action pairs $(o_i,a_i)$, collected from our driving for example. 

The idea behind Imitation Learning is to learn a policy $\pi$ mapping observations to actions, $\pi _{\theta} : \mathcal(O) \rightarrow \mathcal(A)$. The policy would allow the car to know which actions to performs depending on the observation: the camera input, according to the driving we've done before.

The policy can then be trained by minimizing the distance between the policy $\pi _{\theta} (o_t)$ and the expert's actions $a_t$. However, in some cases, the expert's action is not enough. For example, our car might have the possibility to turn left or right in some cases and we need to tell it what it should do: this is Conditional Imitation Learning.

### Conditional Imitation Learning

The framework is the same as above, but we add a commande $c_t$, expressing the expert's intent when the policy is predicting an action to do. The mapping now takes the following form : $\pi _ {\theta} (o_t, c_t)$. The minimization process changes to 
<p align="center">
$\min\limits_{\theta} \displaystyle \sum_{t} \mathcal{L} (\pi _ {\theta} (o_t, c_t),a_t)$
</p>

Let's now dive into the model we used and the dataset we collected :

## Model 



# References :  

1. [Learning to Drive in a day - Wayve](https://arxiv.org/pdf/1807.00412.pdf)
2. [Urban Driving with Conditional Imitation Learning - Wayve](https://arxiv.org/pdf/1912.00177.pdf)
3. [End-to-end Driving via Conditional Imitation Learning - Codevilla et al.](https://arxiv.org/pdf/1710.02410.pdf)

# [Github Repository](to add )